[GitHub - ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post: 365 Days Computer Vision Learning Linkedin Post](https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post)


## Notes from Keras book
We have three main categories of computer vision task done by convnets
1. Classification (single label or multi label)
2. Segmentation 
	1. Semantic (pixels that belong to cat are separated from other classes)
	2. Instance (cat1 vs cat 2)
3. Object detection (bounding box around object)

![](assets/Pasted%20image%2020220706052742.png)

## Resources
- [Computer Vision](https://keras.io/examples/vision/)
- [Investigating Vision Transformer representations](https://keras.io/examples/vision/probing_vits/)
- [GitHub - fchollet/deep-learning-with-python-notebooks: Jupyter notebooks for the code samples of the book "Deep Learning with Python"](https://github.com/fchollet/deep-learning-with-python-notebooks)
- [ApplyingML - Papers, Guides, and Interviews with ML practitioners](https://applyingml.com/): `ApplyingML` collects tacit/tribal/ghost knowledge on applying ML via curated papers/blogs, guides, and interviews with ML practitioners. In a nutshell, it's [1/3 applied-ml, 1/3 ghost knowledge, and 1/3 Tim Ferriss Show](https://applyingml.com/about/). The intent is to make it easier to apply—and benefit from—ML at work. #blog
- [Arthur's DeepCourse: Deep Learning for Computer Vision - AnkiWeb](https://ankiweb.net/shared/info/1910511253) #course
- [Difference between AlexNet, VGGNet, ResNet, and Inception | by Aqeel Anwar | Towards Data Science](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96#:~:text=AlexNet%20and%20ResNet%2D15 2%2C%20both,training%20time%20and%20energy%20required.) #blog
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html#adam) #optimization #dl


## Optimization in machine learning
- [Optimizers](https://keras.io/api/optimizers/) optimizers in keras
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html#adam)
- [Intro to optimization in deep learning: Momentum, RMSProp and Adam](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)  : In this post, we take a look at a problem that plagues training of neural networks, pathological curvature.
- [Gradient Descent Optimization Algorithms - YouTube](https://www.youtube.com/playlist?list=PLreVlKwe2Z0TIZL8Vyfcdw3gKRlB3evGX)  
- [GitHub - averysoh/Gradient-Descent-Variants-Demonstrated-with-Data-Visualisation: Here we attempt to explore the implementation, behaviour and variants of Gradient Descent on complex functions demonstrated through attractive data visualisation](https://github.com/averysoh/Gradient-Descent-Variants-Demonstrated-with-Data-Visualisation)  
- [GitHub - harshraj11584/Paper-Implementation-Overview-Gradient-Descent-Optimization-Sebastian-Ruder: [Python] [arXiv/cs] Paper "An Overview of Gradient Descent Optimization Algorithms" by Sebastian Ruder](https://github.com/harshraj11584/Paper-Implementation-Overview-Gradient-Descent-Optimization-Sebastian-Ruder)  
- [GitHub - averysoh/Gradient-Descent-Variants-Demonstrated-with-Data-Visualisation: Here we attempt to explore the implementation, behaviour and variants of Gradient Descent on complex functions demonstrated through attractive data visualisation](https://github.com/averysoh/Gradient-Descent-Variants-Demonstrated-with-Data-Visualisation)  
- [MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning, Spring 2018 - YouTube](https://m.youtube.com/playlist?list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k)  

## Topics 
- **Normalization methods**: [Normalization Techniques in Deep Neural Networks | by Aakash Bindal | Techspace | Medium](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)
- ![](assets/Pasted%20image%2020220706065713.png)