# Datasets, Software  and research tools for autonomous car
- [GitHub - nutonomy/nuscenes-devkit: The devkit of the nuScenes dataset.](https://github.com/nutonomy/nuscenes-devkit)
- [CARLA Simulator](https://carla.org/) 
- [argoverse](https://www.argoverse.org/)
- [Waymo Open Dataset](https://waymo.com/open/)
- [The latest in Machine Learning | Papers With Code](https://paperswithcode.com/)
- [How to Read More Research Papers?](https://www.louisbouchard.ai/research-papers/)

Use Personal Knowledge Management (you hear Second Brain in other places to refer to this as well) from the get go Obsidian and loqseq are good options
- [Using Logseq PDF annotation and building a research workflow - YouTube](https://www.youtube.com/watch?v=YO02bF1M43E)
- [How I Set Up Obsidian for Academic Work - YouTube](https://www.youtube.com/watch?v=4T0q2kQwc2o)
- [Tiago Forte - YouTube](https://www.youtube.com/channel/UCmvYCRYPDlzSHVNCI_ViJDQ)

# Muzero and why it was succesful

-   [MuZero: Mastering Go, chess, shogi and Atari without rules (blog)](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules)
-   [MuZero Intuition (post)](https://www.furidamu.org/blog/2020/12/22/muzero-intuition/)
-   [MuZero - ICAPS 2020 (30 min talk by author)](https://www.youtube.com/watch?v=L0A86LmH7Yw) [slides](https://drive.google.com/file/d/1nwRRQ3uWXDqX6eth4cHi96OYJ2wuW2Af/edit))
-   [Lessons from AlphaZero Videolecture (video)](https://youtu.be/WQS7933ub9s): A fairly detailed presentation at KTH, Nov. 2021. [Slides from the KTH Lecture](http://web.mit.edu/dimitrib/www/Slides_Lessons%20from%20Alphazero.pdf) that explains in a visual way why MuZero was successful.
-   [LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL (book and accompanying course)](http://web.mit.edu/dimitrib/www/abstractdp_MIT.html) I believe [RLTopics_2022_Lect12.pdf](http://web.mit.edu/dimitrib/www/RLTopics_2022_Lect12.pdf) is specially relevant


# Offline RL
- [Offline Reinforcement Learning](https://neurips.cc/virtual/2021/workshop/21874)  
- [[2005.01643] Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems](https://arxiv.org/abs/2005.01643)  
- [Google Colab](https://colab.research.google.com/drive/1oJOYlAIOl9d1JjlutPY66KmfPkwPCgEE?usp=sharing&authuser=0)  
- [Offline Reinforcement Learning](https://neurips.cc/virtual/2021/workshop/21874)  
- [Offline RL Tutorial - NeurIPS 2020](https://sites.google.com/view/offlinerltutorial-neurips2020/home?authuser=0)


# Inverse RL
## Vidoes  
- [Lecture 6: Inverse Reinforcement Learning -- From Maximum Margin to Maximum Entropy - YouTube](https://www.youtube.com/watch?v=UCWPuyeJ1Kc)  
- [Conference on Robot Learning - YouTube](https://www.youtube.com/channel/UCXnxdtIKJVUN0I-gKo2Chmg)  
- [Imitation Learning: A Series of Deep Dives](https://www.youtube.com/playlist?list=PLQZQ7N26C6ba2BDFVULmmBYC80cX6pNjZ)  
- [Deep RL Bootcamp Lecture 10B Inverse Reinforcement Learning - YouTube](https://www.youtube.com/watch?v=d9DlQSJQAoI)  
- [Conference on Robot Learning - YouTube Inverse Reinforcement Learning Part 1 - YouTube](https://www.youtube.com/watch?v=YFrtqFMglZw)  
## People  
- [About me - Sanjiban Choudhury](http://www.sanjibanchoudhury.com/) works on Inverse RL in aurora  
- [Pieter Abbeel--UC Berkeley--Covariant--Gradescope](https://people.eecs.berkeley.edu/~pabbeel/)  
- [Deep RL Bootcamp: Lectures and Labs](https://sites.google.com/view/deep-rl-bootcamp/lectures) (August 2017)  
- [Full Stack Deep Learning Bootcamp](https://course.fullstackdeeplearning.com/) (2020) with Josh Tobin and Sergey Karayev  
  
## Papers  
- [[2012.00889] Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and  
[Fetching Title#hzxn](https://arxiv.org/abs/1806.06877)Algorithms](https://arxiv.org/abs/2012.00889)  
- [Feedback in Imitation Learning: The Three Regimes of Covariate Shift](https://arxiv.org/pdf/2102.02872.pdf)  
- [133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization](http://makerhacker.github.io/paper-mining/nips/nips2008/nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html)  
- [[2109.13333] Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients](https://arxiv.org/abs/2109.13333)  
- [[1510.09142] Learning Continuous Control Policies by Stochastic Value Gradients](https://arxiv.org/abs/1510.09142)  
- [[1806.06877] A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress](https://arxiv.org/abs/1806.06877)  
- [Favorite papers from 2021 : reinforcementlearning](https://www.reddit.com/r/reinforcementlearning/comments/rr8thh/favorite_papers_from_2021/)  
- [https://arxiv.org/pdf/2201.06539.pdf](https://arxiv.org/pdf/2201.06539.pdf)  
  
## Books  
- [Artificial Intelligence: Foundations of Computational Agents,  2nd Edition](http://artint.info/2e/html/ArtInt2e.html)  
- An Algorithmic Perspective on  
Imitation Learning: [https://arxiv.org/ftp/arxiv/papers/1811/1811.06711.pdf](https://arxiv.org/ftp/arxiv/papers/1811/1811.06711.pdf)  
  
## Courses  
- - [Imitation Learning: A Series of Deep Dives](https://www.youtube.com/playlist?list=PLQZQ7N26C6ba2BDFVULmmBYC80cX6pNjZ)  
- [MIT 6.S090 - Deep Learning for Control](https://pulkitag.github.io/rlbootcamp-iap/)  
- [Deep RL Bootcamp - Lectures](https://sites.google.com/view/deep-rl-bootcamp/lectures)  
- [Reinforcement Learning Crash Course](https://rlcourse.com/).  
- [Theory of Reinforcement Learning Boot Camp | Simons Institute for the Theory of Computing](https://simons.berkeley.edu/workshops/rl-2020-bc)  
  
## Concepts  
- Policy vs. Value Iteration  
- [Policy and Value Iteration. An Introduction to Reinforcement… | by Steve Roberts | Towards Data Science](https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2)  
- [Artificial Intelligence - foundations of computational agents -- 9.5.3 Value Iteration](https://artint.info/html/ArtInt_227.html)  
- [Artificial Intelligence - foundations of computational agents -- 9.5.4 Policy Iteration](https://artint.info/html/ArtInt_228.html)  
  
## Learn  
#learn  
  
**Q:** What problem do Value Iteration Networks solve  
??  
- We can generalize.  
- Can solve problems that need planning rather than reactive action selection  
- We embed a CNN that acts as a Value Iteration step and since it is differentiable, we can train the whole network and be able to solve planning and generalizing issues  
- Using the approach, Tamar et al. show that value iteration networks (VINS) generalize better to new grid-world scenarios than either CNNs following the [DQN](https://blog.acolyer.org/2016/03/02/graying-the-black-box-understanding-dqns/) architecture, or fully convolutional networks (FCNs):  
- [Fundamental Iterative Methods of Reinforcement Learning | by Nathan Lambert | Towards Data Science](https://towardsdatascience.com/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a)  
- Ref [Value iteration networks | the morning paper](https://blog.acolyer.org/2017/02/09/value-iteration-networks/)  
    
**Q:** In Imitation Learning, one problem is focusing on latest data and ignoring the previously explored policies. We might end up switching back and forth between two policies without knowing that we have already explored them. What is the remedy to this problem  
??  
No Regret Online Learning  
  
**Q:** What is on-policy vs off-policy in RL  
??  
- **On-policy**: Use the deterministic outcomes or samples from the target policy to train the algorithm.  
- **Off-policy**: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.  
Ref: **[A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts)**  
  
**Q:** What are the Policy Gradient Methods in RL  
??  
We create a parametrized policy and can take derivatives of the loss function with respect to this parameter and thus update the policy  
- [Policy gradient methods - Scholarpedia](http://www.scholarpedia.org/article/Policy_gradient_methods)  
- [Policy Gradients in a Nutshell. Everything you need to know to get… | by Sanyam Kapoor | Towards Data Science](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)  
- [RL — Policy Gradient Explained. Policy Gradient Methods (PG) are… | by Jonathan Hui | Medium](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)  
- [[https://stanford.edu/~ashlearn/RLForFinanceBook/PolicyGradient.pdf](https://stanford.edu/~ashlearn/RLForFinanceBook/PolicyGradient.pdf)](https://stanford.edu/~ashlearn/RLForFinanceBook/PolicyGradient.pdf](https://stanford.edu/~ashlearn/RLForFinanceBook/PolicyGradient.pdf))